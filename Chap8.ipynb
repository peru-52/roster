{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6MSwWIkA/4y6Oi/yuWd8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peru-52/roster/blob/main/Chap8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiX_qDHXCcTX",
        "outputId": "3dc67866-ceb5-4243-af74-b0cb4c59aa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.17.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras-hub==0.17.0 (from keras_nlp)\n",
            "  Downloading keras_hub-0.17.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (2024.9.11)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (13.9.4)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (0.3.4)\n",
            "Collecting tensorflow-text (from keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (4.66.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (4.12.2)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text->keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.67.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.5.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.45.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.0.2)\n",
            "Downloading keras_nlp-0.17.0-py3-none-any.whl (2.0 kB)\n",
            "Downloading keras_hub-0.17.0-py3-none-any.whl (644 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow-text, keras-hub, keras_nlp\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-hub-0.17.0 keras_nlp-0.17.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEB-M1GLCvK5",
        "outputId": "565a5644-5edb-493a-af02-c204bd71710e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"]=\"tensorflow\"\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "ScS3xBlzC23d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfNytv0zC59Y",
        "outputId": "847df5d7-ed2c-40ae-c602-aec77b9f871d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=178028cca1bf25cb0668203f699a06a2062e76f47d3e799c96b5899ae3280a61\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\""
      ],
      "metadata": {
        "id": "jaPWYWmDC8NH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "filename = wget.download(url)\n",
        "filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "74RJ4sMADAce",
        "outputId": "fc1923a1-a91e-493a-9d6d-e00248d8103f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ja.text8.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ja.text8.zip"
      ],
      "metadata": {
        "id": "kDFX_sD6DCu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c86d24-1ed5-4d4c-8fff-53c984eddeaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ja.text8.zip\n",
            "  inflating: ja.text8                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "def load_data(filepath, encoding='utf-8'):\n",
        " with open(filepath, encoding=encoding) as f:\n",
        "  return f.read()"
      ],
      "metadata": {
        "id": "JzBqnm1QDFNR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
        "\n",
        "def build_vocabulary(text, num_words=None):\n",
        " tokenizer = Tokenizer(\n",
        "    num_words=num_words,\n",
        "    oov_token='<unk>'\n",
        " )\n",
        " tokenizer.fit_on_texts([text])\n",
        " return tokenizer\n",
        "\n",
        "def create_dataset(text, vocab, num_words, window_size,negative_samples):\n",
        " data = vocab.texts_to_sequences([text]).pop()\n",
        " sampling_table = make_sampling_table(num_words)\n",
        "\n",
        " couples, labels = skipgrams(data, num_words,\n",
        "                             window_size=window_size,\n",
        "                             negative_samples=negative_samples,\n",
        "                             sampling_table=sampling_table)\n",
        " word_target, word_context = zip(*couples)\n",
        " word_target = np.reshape(word_target, (-1,1))\n",
        " word_context = np.reshape(word_context, (-1,1))\n",
        " labels = np.asarray(labels)\n",
        " return [word_target, word_context], labels"
      ],
      "metadata": {
        "id": "yVqiAMgbDJwx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dot, Flatten, Embedding, Dense"
      ],
      "metadata": {
        "id": "HazERcVzDNQ2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        " def __init__(self, vocab_size, emb_dim=100):\n",
        "    self.word_input = Input(shape=(1,), name='word_input')\n",
        "    self.word_embed = Embedding(input_dim=vocab_size,\n",
        "                                output_dim=emb_dim,\n",
        "                                input_length=1,\n",
        "                                name='word_embedding')\n",
        "    self.context_input = Input(shape=(1,), name='context_input')\n",
        "    self.context_embed = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=emb_dim,\n",
        "                                   input_length=1,\n",
        "                                   name='context_embedding')\n",
        "    self.dot = Dot(axes=2)\n",
        "    self.flatten = Flatten()\n",
        "    self.output = Dense(1, activation='sigmoid')\n",
        "\n",
        " def build(self):\n",
        "    word_embed = self.word_embed(self.word_input)\n",
        "    context_embed = self.context_embed(self.context_input)\n",
        "    dot = self.dot([word_embed, context_embed])\n",
        "    flatten = self.flatten(dot)\n",
        "    output = self.output(flatten)\n",
        "    model = Model(inputs=[self.word_input, self.context_input],\n",
        "                  outputs=output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nIwaSZhxDP04"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class InferenceAPI:\n",
        "  def __init__(self, model, vocab):\n",
        "      self.vocab = vocab\n",
        "      self.weights = model.get_layer('word_embedding').get_weights()[0]\n",
        "\n",
        "  def most_similar(self, word, topn=10):\n",
        "      word_index = self.vocab.word_index.get(word, 1)\n",
        "      sim = self._cosine_similarity(word_index)\n",
        "      pairs = [(s, i) for i, s in enumerate(sim)]\n",
        "      pairs.sort(reverse=True)\n",
        "      pairs = pairs[1: topn + 1]\n",
        "      res = [(self.vocab.index_word[i], s) for s, i in pairs]\n",
        "      return res\n",
        "\n",
        "  def similarity(self, word1, word2):\n",
        "      word_index1 = self.vocab.word_index.get(word1, 1)\n",
        "      word_index2 = self.vocab.word_index.get(word2, 1)\n",
        "      weight1 = self.weights[word_index1]\n",
        "      weight2 = self.weights[word_index2]\n",
        "      return cosine(weight1, weight2)\n",
        "\n",
        "  def _cosine_similarity(self, target_index):\n",
        "      target_weight = self.weights[target_index]\n",
        "      similarity = cosine_similarity(self.weights, [target_weight])\n",
        "      return similarity.flatten()"
      ],
      "metadata": {
        "id": "mutdyZMpDYtY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "cBomMM6dDVXU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 50\n",
        "epochs = 10\n",
        "model_path = 'model.keras'\n",
        "negative_samples = 1\n",
        "num_words = 10000\n",
        "window_size = 1"
      ],
      "metadata": {
        "id": "2fkL2MtIDcQh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = load_data(filepath='ja.text8')\n",
        "\n",
        "vocab = build_vocabulary(text, num_words)\n",
        "\n",
        "x, y = create_dataset(text, vocab, num_words,\n",
        "                      window_size, negative_samples)\n",
        "\n",
        "model = EmbeddingModel(num_words, emb_dim)\n",
        "model = model.build()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "callbacks = [\n",
        " EarlyStopping(patience=1),\n",
        " ModelCheckpoint(model_path, save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(x=x,\n",
        "          y=y,\n",
        "          batch_size=128,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "yDoYaoQjDibt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9168d777-7943-4fe8-874d-be60cff0df15"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['word_input', 'context_input']. Received: the structure of inputs=('*', '*')\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2ms/step - loss: 0.2884 - val_loss: 0.2030\n",
            "Epoch 2/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2ms/step - loss: 0.1855 - val_loss: 0.1862\n",
            "Epoch 3/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2ms/step - loss: 0.1661 - val_loss: 0.1789\n",
            "Epoch 4/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2ms/step - loss: 0.1558 - val_loss: 0.1750\n",
            "Epoch 5/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2ms/step - loss: 0.1487 - val_loss: 0.1733\n",
            "Epoch 6/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2ms/step - loss: 0.1433 - val_loss: 0.1712\n",
            "Epoch 7/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2ms/step - loss: 0.1391 - val_loss: 0.1699\n",
            "Epoch 8/10\n",
            "\u001b[1m59582/59582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2ms/step - loss: 0.1363 - val_loss: 0.1711\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ca54e2cf1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "# モデルとAPIのロード\n",
        "model = load_model('model.keras')  # 学習済みモデルのロード\n",
        "api = InferenceAPI(model, vocab)  # 類似度計算APIのインスタンス作成\n",
        "\n",
        "# 5つの単語に共通する類似単語を取得する関数\n",
        "def get_common_similar_words(api, words, topn=10):\n",
        "    try:\n",
        "        # 各単語の埋め込みを取得\n",
        "        embeddings = []\n",
        "        for word in words:\n",
        "            word_index = api.vocab.word_index.get(word, None)\n",
        "            if word_index is not None:\n",
        "                embeddings.append(api.weights[word_index])\n",
        "            else:\n",
        "                print(f\"単語 '{word}' はボキャブラリに存在しません。無視します。\")\n",
        "\n",
        "        if not embeddings:\n",
        "            print(\"有効な単語が入力されていません。\")\n",
        "            return\n",
        "\n",
        "        # ベクトルの平均を計算\n",
        "        average_embedding = np.mean(embeddings, axis=0)\n",
        "\n",
        "        # 平均ベクトルに基づいて類似単語を取得\n",
        "        similarity = cosine_similarity(api.weights, [average_embedding]).flatten()\n",
        "        most_similar_indices = similarity.argsort()[-topn-1:][::-1][1:]  # 自身を除外\n",
        "\n",
        "        # 類似単語を取得\n",
        "        result = [(api.vocab.index_word[idx], similarity[idx]) for idx in most_similar_indices]\n",
        "        pprint(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"エラーが発生しました: {e}\")\n",
        "\n",
        "# ユーザー入力\n",
        "print(\"5つの単語を入力してください（スペース区切り）。\")\n",
        "input_words = input(\"入力: \").strip().split()\n",
        "\n",
        "if len(input_words) == 5:  # 5つの単語が入力された場合のみ実行\n",
        "    get_common_similar_words(api, input_words, topn=10)\n",
        "else:\n",
        "    print(\"エラー: 必ず5つの単語を入力してください！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax6oBv0Wn4KU",
        "outputId": "2fad13f6-c7d8-4ceb-ffb1-24c9c99f9741"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5つの単語を入力してください（スペース区切り）。\n",
            "入力: 猫 犬 狼 豚 羊\n",
            "[('猫', 0.9204491),\n",
            " ('羊', 0.90933365),\n",
            " ('蛇', 0.9058565),\n",
            " ('クモ', 0.8949733),\n",
            " ('鳥', 0.88749444),\n",
            " ('爪', 0.88749),\n",
            " ('狼', 0.8844025),\n",
            " ('影', 0.87872165),\n",
            " ('夢', 0.8773711),\n",
            " ('破', 0.87658095)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# モデルとAPIのロード\n",
        "model = load_model('model.keras')  # 学習済みモデルのロード\n",
        "api = InferenceAPI(model, vocab)  # 類似度計算APIのインスタンス作成\n",
        "\n",
        "# 類似単語を取得する関数\n",
        "def display_similar_words(api, words, topn=10):\n",
        "    for word in words:\n",
        "        print(f\"\\n【単語】: {word}\")\n",
        "        try:\n",
        "            similar_words = api.most_similar(word=word, topn=topn)\n",
        "            pprint(similar_words)  # 類似単語を出力\n",
        "        except KeyError:\n",
        "            print(f\"単語 '{word}' はボキャブラリに存在しません。\")\n",
        "\n",
        "# ユーザー入力\n",
        "print(\"5つの単語を入力してください（スペース区切り）。\")\n",
        "input_words = input(\"入力: \").strip().split()\n",
        "\n",
        "if len(input_words) == 5:  # 5つの単語が入力された場合のみ実行\n",
        "    display_similar_words(api, input_words, topn=10)\n",
        "else:\n",
        "    print(\"エラー: 必ず5つの単語を入力してください！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF93bCxLXFy4",
        "outputId": "d7007e39-6dec-445f-a1d2-809190c742f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5つの単語を入力してください（スペース区切り）。\n",
            "入力: 猫 犬 狼 豚 羊\n",
            "\n",
            "【単語】: 猫\n",
            "[('唄', 0.8827321),\n",
            " ('蛇', 0.87987155),\n",
            " ('天井', 0.8657071),\n",
            " ('我', 0.85345125),\n",
            " ('鳥', 0.8534021),\n",
            " ('皮', 0.8444024),\n",
            " ('爪', 0.84416175),\n",
            " ('精', 0.8431183),\n",
            " ('ウサギ', 0.84307057),\n",
            " ('クモ', 0.84300303)]\n",
            "\n",
            "【単語】: 犬\n",
            "[('豚', 0.8063334),\n",
            " ('恐竜', 0.79668355),\n",
            " ('宇宙船', 0.78453475),\n",
            " ('クモ', 0.7818955),\n",
            " ('物体', 0.7784943),\n",
            " ('部屋', 0.77039504),\n",
            " ('洞窟', 0.7569414),\n",
            " ('銀', 0.7540104),\n",
            " ('事典', 0.7538121),\n",
            " ('爪', 0.7530182)]\n",
            "\n",
            "【単語】: 狼\n",
            "[('幻', 0.8762877),\n",
            " ('姫', 0.8597362),\n",
            " ('雪', 0.85711294),\n",
            " ('蛇', 0.8565743),\n",
            " ('恋', 0.85608166),\n",
            " ('剣', 0.85352784),\n",
            " ('太夫', 0.8530679),\n",
            " ('かれ', 0.8485142),\n",
            " ('霧', 0.84840065),\n",
            " ('不動', 0.8475653)]\n",
            "\n",
            "【単語】: 豚\n",
            "[('猫', 0.8424875),\n",
            " ('魚', 0.8104037),\n",
            " ('鳥', 0.80754024),\n",
            " ('犬', 0.8063334),\n",
            " ('皮', 0.8027852),\n",
            " ('棺', 0.8015492),\n",
            " ('羊', 0.8000477),\n",
            " ('涙', 0.7998967),\n",
            " ('クモ', 0.79908043),\n",
            " ('天井', 0.79722905)]\n",
            "\n",
            "【単語】: 羊\n",
            "[('母', 0.85534763),\n",
            " ('蛇', 0.85423636),\n",
            " ('張', 0.84902734),\n",
            " ('鹿', 0.8472137),\n",
            " ('雪', 0.84695554),\n",
            " ('蝶', 0.84272975),\n",
            " ('紫', 0.84147406),\n",
            " ('富', 0.84045196),\n",
            " ('瑞', 0.83362436),\n",
            " ('姫', 0.83350235)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#考察\n",
        "今回は動物の単語を入れたため、同じように動物を表す単語が出力されると考えた。しかし、実際には動物とは関係がない単語も表示された。動物と関連強い単語が出力されていると考えた。しかし、動物に関連する単語の出力もあった。爪や皮などは動物を形成する一部分を表す単語である。「猫」が一番最初に入力されているため、猫が与える影響が大きいと考えた。\n",
        "単語ごとだと、より入力された単語から連想されやすい単語が出力されていると考えた。"
      ],
      "metadata": {
        "id": "V1oclPFmXYK1"
      }
    }
  ]
}